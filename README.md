# Adobe India Hackathon - README Collection

This document includes **both** README files for Round 1A and Round 1B solutions of the Adobe "Connecting the Dots" Hackathon. These can be used directly in their respective project folders or in a combined GitHub repo.

---

## ğŸ§  Round 1A â€“ PDF Outline Extractor

> **Track:** Connecting the Dots
> **Challenge:** Automatically extract structured outlines from unstructured PDFs

### ğŸ“Œ Overview

This solution extracts structured outlines from input PDF documents. Each output contains:

* ğŸ·ï¸ Title of the document
* ğŸ§© Headings (H1, H2, H3)
* ğŸ“„ Corresponding page numbers

### ğŸ’¡ Approach

We implemented a **font-style-aware heuristic algorithm**:

* **Font Metadata Parsing**: Extracts size, font name, and position
* **Heading Detection**: Clusters font sizes to infer heading levels
* **Title Detection**: Picks the largest text on the first page

### ğŸ› ï¸ Tools Used

* `Python 3.8+`
* `PyMuPDF (fitz)`
* `json` (built-in)

### ğŸ“ Folder Structure

```
adobe-round1a-outline/
â”œâ”€â”€ input/               â† Input PDFs
â”œâ”€â”€ output/              â† Output JSONs
â”œâ”€â”€ main.py              â† Main script
â”œâ”€â”€ utils.py             â† Logic helpers
â”œâ”€â”€ requirements.txt     â† Python dependencies
â”œâ”€â”€ Dockerfile           â† (Optional) container setup
â””â”€â”€ README.md            â† This file
```

### ğŸ§ª How to Run

```bash
python -m venv venv
source venv/bin/activate       # Windows: venv\Scripts\activate
pip install -r requirements.txt
python main.py
```

### ğŸ“ Sample Output Format

```json
{
  "title": "Document Title",
  "headings": [
    {
      "text": "Introduction",
      "level": 1,
      "page_number": 1
    }
  ]
}
```

### ğŸ”® Future Work

* KMeans clustering of font styles
* NLP-based heading validation
* Multi-column layout support

---

## ğŸ“Š Round 1B â€“ Multi-Collection PDF Persona Analysis

> **Track:** Connecting the Dots
> **Challenge:** Analyze and extract semantically relevant sections from PDFs across multiple collections based on a personaâ€™s need.

### ğŸ“Œ Objective

Given multiple PDFs and a user persona + task, extract the most relevant sections and refined content, and return them ranked by importance.

### ğŸ§  Approach Summary

* Load JSON input and PDFs
* Extract headings (reuses Round 1A logic)
* Use `sentence-transformers` to embed headings, persona, and job
* Compute similarity scores â†’ rank sections
* Extract best-matching pages/snippets

### ğŸ› ï¸ Stack

| Task             | Tool                    |
| ---------------- | ----------------------- |
| PDF Parsing      | `PyMuPDF` (fitz)        |
| Text Embeddings  | `sentence-transformers` |
| Scoring          | `cosine_similarity`     |
| Containerization | Docker (offline mode)   |

### ğŸ“ Structure

```
Round1B/
â”œâ”€â”€ Collection_1/â€¦     â† 3 collection folders
â”œâ”€â”€ app/               â† logic scripts
â”œâ”€â”€ model/             â† MiniLM model cache
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ approach_explanation.md
```

### ğŸ“¤ Output JSON

```json
{
  "metadata": {
    "persona": "HR Professional",
    "job_to_be_done": "Design onboarding forms"
  },
  "extracted_sections": [
    {
      "document": "form_guide.pdf",
      "section_title": "Creating Fillable Forms",
      "importance_rank": 1,
      "page_number": 3
    }
  ],
  "subsection_analysis": [
    {
      "document": "form_guide.pdf",
      "refined_text": "You can create fillable text fields using...",
      "page_number": 3
    }
  ]
}
```

### ğŸ³ Docker Usage

```bash
# Build
docker build --platform linux/amd64 -t round1b-solution .

# Run
docker run --rm \
  -v ${PWD}/Collection_1:/app/Collection_1 \
  --network none \
  round1b-solution
```

---


